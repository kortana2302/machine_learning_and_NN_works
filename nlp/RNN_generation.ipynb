{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1555,
     "status": "ok",
     "timestamp": 1620559384949,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "zKMq7dp2W15Y",
    "outputId": "50cd1b72-e83f-4aa7-d553-533bf1961802"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\79165\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmWCBWxrBUB3"
   },
   "source": [
    "## 1. Генерирование русских имен при помощи RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "990obDBwCC7V"
   },
   "source": [
    "Датасет: https://disk.yandex.ru/i/2yt18jHUgVEoIw\n",
    "\n",
    "1.1 На основе файла name_rus.txt создайте датасет.\n",
    "  * Учтите, что имена могут иметь различную длину\n",
    "  * Добавьте 4 специальных токена: \n",
    "    * `<PAD>` для дополнения последовательности до нужной длины;\n",
    "    * `<UNK>` для корректной обработки ранее не встречавшихся токенов;\n",
    "    * `<SOS>` для обозначения начала последовательности;\n",
    "    * `<EOS>` для обозначения конца последовательности.\n",
    "  * Преобразовывайте строку в последовательность индексов с учетом следующих замечаний:\n",
    "    * в начало последовательности добавьте токен `<SOS>`;\n",
    "    * в конец последовательности добавьте токен `<EOS>` и, при необходимости, несколько токенов `<PAD>`;\n",
    "  * `Dataset.__get_item__` возращает две последовательности: последовательность для обучения и правильный ответ. \n",
    "  \n",
    "  Пример:\n",
    "  ```\n",
    "  s = 'The cat sat on the mat'\n",
    "  # преобразуем в индексы\n",
    "  s_idx = [2, 5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
    "  # получаем x и y (__getitem__)\n",
    "  x = [2, 5, 1, 2, 8, 4, 7, 3, 0]\n",
    "  y = [5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
    "  ```\n",
    "\n",
    "1.2 Создайте и обучите модель для генерации фамилии.\n",
    "\n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`;\n",
    "  * Используйте рекуррентные слои;\n",
    "  * Задача ставится как предсказание следующего токена в каждом примере из пакета для каждого момента времени. Т.е. в данный момент времени по текущей подстроке предсказывает следующий символ для данной строки (задача классификации);\n",
    "  * Примерная схема реализации метода `forward`:\n",
    "  ```\n",
    "    input_X: [batch_size x seq_len] -> nn.Embedding -> emb_X: [batch_size x seq_len x embedding_size]\n",
    "    emb_X: [batch_size x seq_len x embedding_size] -> nn.RNN -> output: [batch_size x seq_len x hidden_size] \n",
    "    output: [batch_size x seq_len x hidden_size] -> torch.Tensor.reshape -> output: [batch_size * seq_len x hidden_size]\n",
    "    output: [batch_size * seq_len x hidden_size] -> nn.Linear -> output: [batch_size * seq_len x vocab_size]\n",
    "  ```\n",
    "\n",
    "1.3 Напишите функцию, которая генерирует фамилию при помощи обученной модели:\n",
    "  * Построение начинается с последовательности единичной длины, состоящей из индекса токена `<SOS>`;\n",
    "  * Начальное скрытое состояние RNN `h_t = None`;\n",
    "  * В результате прогона последнего токена из построенной последовательности через модель получаете новое скрытое состояние `h_t` и распределение над всеми токенами из словаря;\n",
    "  * Выбираете 1 токен пропорционально вероятности и добавляете его в последовательность (можно воспользоваться `torch.multinomial`);\n",
    "  * Повторяете эти действия до тех пор, пока не сгенерирован токен `<EOS>` или не превышена максимальная длина последовательности.\n",
    "\n",
    "При обучении каждые `k` эпох генерируйте несколько фамилий и выводите их на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>авдокея</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>авдоким</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>авдоня</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>авдотька</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>авдотьюшка</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0     авдокея\n",
       "1     авдоким\n",
       "2      авдоня\n",
       "3    авдотька\n",
       "4  авдотьюшка"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table('name_rus.txt', encoding = 'windows-1251', header = None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "  def __init__(self, data):\n",
    "    self.idx_to_token = dict()\n",
    "    self.token_to_idx = dict()\n",
    "    self.token_to_idx['<PAD>'] = 0\n",
    "    self.idx_to_token[0] = '<PAD>'\n",
    "    self.token_to_idx['<UNK>'] = 1\n",
    "    self.idx_to_token[1] = '<UNK>'\n",
    "    self.token_to_idx['<SOS>'] = 2\n",
    "    self.idx_to_token[2] = '<SOS>'\n",
    "    self.token_to_idx['<EOS>'] = 3\n",
    "    self.idx_to_token[3] = '<EOS>'\n",
    "    self.max_seq_len = 0\n",
    "    k = 4\n",
    "    for sur in data.values:\n",
    "        for w in sur.lower():\n",
    "            if w not in self.token_to_idx:\n",
    "                self.token_to_idx[w] = k\n",
    "                self.idx_to_token[k] = w\n",
    "                k+=1\n",
    "        if len(sur)>self.max_seq_len:\n",
    "                self.max_seq_len = len(sur)+2\n",
    "    self.vocab_len = len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesDataset(Dataset):\n",
    "  def __init__(self, vocab):\n",
    "    self.vocab = vocab\n",
    "\n",
    "  def vectorize(self, surname):\n",
    "    surname_rep = []\n",
    "    for sur in surname:\n",
    "        rep = []\n",
    "        for w in sur.lower():\n",
    "            rep.append(self.vocab.token_to_idx[w])\n",
    "        a = [vocab.token_to_idx['<SOS>']]+rep+[vocab.token_to_idx['<EOS>']]+[0]*(vocab.max_seq_len-len(rep)-2)\n",
    "        surname_rep.append(torch.tensor(a))\n",
    "    self.X_rep = torch.stack(surname_rep)\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.X_rep)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample = self.X_rep[idx], torch.tensor(list(self.X_rep[idx][1:])+[0])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SurnamesDataset(vocab)\n",
    "dataset.vectorize(df[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2,  4,  5,  6,  7,  8,  9, 10,  3,  0,  0,  0,  0,  0,  0]),\n",
       " tensor([ 4,  5,  6,  7,  8,  9, 10,  3,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = next(iter(dataset))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = torch.utils.data.random_split(dataset, [int(len(dataset)*0.81), len(dataset) - int(len(dataset)*0.81)], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_train, batch_size = 14, shuffle = True)\n",
    "test_loader = DataLoader(data_test, batch_size = 14, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rnn(model):\n",
    "    with torch.no_grad():  \n",
    "        inp = [2]\n",
    "        hidden = torch.zeros((1, 300))\n",
    "\n",
    "        output_name = ''\n",
    "\n",
    "        for i in range(vocab.max_seq_len):\n",
    "            output, hidden = model(inp, hidden)\n",
    "            idx = output.argmax().item()\n",
    "            if idx==3:\n",
    "                print(output_name)\n",
    "                return\n",
    "            output_name += vocab.idx_to_token[idx]\n",
    "            inp = [idx]\n",
    "        print(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss = 1.23848795, test_loss = 1.02076060\n",
      "Точность на тестовой выборке: 0.6862433862433862\n",
      "марина\n",
      "epoch 5: train_loss = 0.79634791, test_loss = 0.87443042\n",
      "Точность на тестовой выборке: 0.7199294532627866\n",
      "милина\n",
      "epoch 10: train_loss = 0.71433511, test_loss = 0.82541613\n",
      "Точность на тестовой выборке: 0.736331569664903\n",
      "степаня\n",
      "epoch 15: train_loss = 0.66994083, test_loss = 0.81562002\n",
      "Точность на тестовой выборке: 0.7430335097001763\n",
      "манюша\n",
      "epoch 20: train_loss = 0.64580120, test_loss = 0.80504597\n",
      "Точность на тестовой выборке: 0.7440917107583774\n",
      "марина\n",
      "epoch 25: train_loss = 0.62841837, test_loss = 0.81414111\n",
      "Точность на тестовой выборке: 0.7463844797178131\n",
      "митуся\n",
      "epoch 30: train_loss = 0.61906879, test_loss = 0.80868988\n",
      "Точность на тестовой выборке: 0.7488536155202822\n",
      "марга\n",
      "epoch 35: train_loss = 0.60844102, test_loss = 0.82136437\n",
      "Точность на тестовой выборке: 0.7463844797178131\n",
      "миленка\n",
      "epoch 40: train_loss = 0.60600944, test_loss = 0.81725389\n",
      "Точность на тестовой выборке: 0.7537918871252205\n",
      "мариамна\n",
      "epoch 45: train_loss = 0.59694191, test_loss = 0.83991310\n",
      "Точность на тестовой выборке: 0.7507936507936508\n",
      "веля\n",
      "epoch 50: train_loss = 0.59213975, test_loss = 0.83021217\n",
      "Точность на тестовой выборке: 0.7530864197530864\n",
      "мариана\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.rnn = nn.RNN(input_size = input_size, hidden_size=hidden_size, batch_first=True)\n",
    "    self.fc1 = nn.Linear(hidden_size, 100)\n",
    "    self.fc2 = nn.Linear(100, output_size)\n",
    "    self.emb = nn.Embedding(vocab.vocab_len, input_size)\n",
    "    \n",
    "  def forward(self, X, h):\n",
    "    X = self.emb(torch.LongTensor(X))\n",
    "    X, h = self.rnn(X, h)\n",
    "    h = h.detach()\n",
    "    if len(X.shape)==3:\n",
    "        X = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "    X = nn.ReLU()(self.fc1(F.dropout(X, 0.3)))\n",
    "    y_pred = self.fc2(F.dropout(X, 0.3))\n",
    "    return y_pred, h\n",
    "\n",
    "h = torch.zeros((1, 14, 300), requires_grad=True)\n",
    "model = Model(200, 300, vocab.vocab_len)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i in range(51):\n",
    "    model.train()\n",
    "    losses_epoch_train = []\n",
    "    losses_epoch_test = []\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "    for x_example, y_example in train_loader:\n",
    "        y_pred, h = model(x_example, h)\n",
    "    \n",
    "        loss = criterion(y_pred, y_example.long().reshape(1,-1)[0])\n",
    "        losses_epoch_train.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    for x_example, y_example in test_loader:\n",
    "        y_pred, h = model(x_example, h)\n",
    "    \n",
    "        loss = criterion(y_pred, y_example.long().reshape(1,-1)[0])\n",
    "        losses_epoch_test.append(loss.item())\n",
    "        \n",
    "        y_pred_test.extend(y_pred)\n",
    "        y_true_test.extend(list(y_example))\n",
    "    \n",
    "    train_losses.append(np.mean(losses_epoch_train))\n",
    "    test_losses.append(np.mean(losses_epoch_test))\n",
    "    y_pred_test = list(map(lambda x: torch.argmax(x).item(), y_pred_test))\n",
    "    y_true_test = list(map(lambda x: x.item(), torch.stack(y_true_test).reshape(-1,1)))\n",
    "    if i%5==0:\n",
    "        print(f'epoch {i}: train_loss = {np.mean(losses_epoch_train):.8f}, test_loss = {np.mean(losses_epoch_test):.8f}') \n",
    "        print('Точность на тестовой выборке:', accuracy_score(y_true_test, y_pred_test))\n",
    "        sample_rnn(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_lstm():\n",
    "    with torch.no_grad(): \n",
    "        inp = [2]\n",
    "        h = torch.zeros((1, 100))\n",
    "        c = torch.zeros((1, 100))\n",
    "\n",
    "        output_name = ''\n",
    "\n",
    "        for i in range(vocab.max_seq_len):\n",
    "            prev_state = (h,c)\n",
    "            output, (h, c) = model(inp, prev_state)\n",
    "            idx = output.argmax().item()\n",
    "            if idx==3:\n",
    "                print(output_name)\n",
    "                return\n",
    "            output_name += vocab.idx_to_token[idx]\n",
    "            inp = [idx]\n",
    "        print(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss = 1.49092728, test_loss = 1.10987705\n",
      "Точность на тестовой выборке: 0.6724867724867725\n",
      "линя\n",
      "epoch 5: train_loss = 0.85844934, test_loss = 0.91496675\n",
      "Точность на тестовой выборке: 0.708289241622575\n",
      "ана\n",
      "epoch 10: train_loss = 0.78250860, test_loss = 0.84787964\n",
      "Точность на тестовой выборке: 0.7319223985890653\n",
      "андрий\n",
      "epoch 15: train_loss = 0.73421566, test_loss = 0.83577189\n",
      "Точность на тестовой выборке: 0.7379188712522046\n",
      "алексина\n",
      "epoch 20: train_loss = 0.70655729, test_loss = 0.80824548\n",
      "Точность на тестовой выборке: 0.7396825396825397\n",
      "ма\n",
      "epoch 25: train_loss = 0.68379396, test_loss = 0.81633204\n",
      "Точность на тестовой выборке: 0.7393298059964727\n",
      "андрина\n",
      "epoch 30: train_loss = 0.66696342, test_loss = 0.79976082\n",
      "Точность на тестовой выборке: 0.746031746031746\n",
      "марина\n",
      "epoch 35: train_loss = 0.65803130, test_loss = 0.80094813\n",
      "Точность на тестовой выборке: 0.7458553791887125\n",
      "милий\n",
      "epoch 40: train_loss = 0.64335950, test_loss = 0.80999361\n",
      "Точность на тестовой выборке: 0.7477954144620811\n",
      "маля\n",
      "epoch 45: train_loss = 0.63714747, test_loss = 0.82461168\n",
      "Точность на тестовой выборке: 0.7455026455026456\n",
      "маринка\n",
      "epoch 50: train_loss = 0.62899672, test_loss = 0.81393609\n",
      "Точность на тестовой выборке: 0.7439153439153439\n",
      "маринка\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.rnn = nn.LSTM(input_size = input_size, hidden_size=hidden_size, batch_first=True)\n",
    "    self.fc1 = nn.Linear(hidden_size, 100)\n",
    "    self.fc2 = nn.Linear(100, output_size)\n",
    "    self.emb = nn.Embedding(vocab.vocab_len, input_size)\n",
    "    \n",
    "  def forward(self, X, prev_state):\n",
    "    X = self.emb(torch.LongTensor(X))\n",
    "    X, (h, c) = self.rnn(X, prev_state)\n",
    "    h = h.detach()\n",
    "    c = c.detach()\n",
    "    if len(X.shape)==3:\n",
    "        X = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "    X = nn.ReLU()(self.fc1(F.dropout(X, 0.3)))\n",
    "    y_pred = self.fc2(F.dropout(X, 0.3))\n",
    "    return y_pred, (h, c)\n",
    "\n",
    "h = torch.zeros((1, 14, 100), requires_grad=True)\n",
    "c = torch.zeros((1, 14, 100), requires_grad=True)\n",
    "model = Model(200, 100, vocab.vocab_len)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i in range(51):\n",
    "    model.train()\n",
    "    losses_epoch_train = []\n",
    "    losses_epoch_test = []\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "    for x_example, y_example in train_loader:\n",
    "        y_pred, (h, c) = model(x_example, (h, c))\n",
    "    \n",
    "        loss = criterion(y_pred, y_example.long().reshape(1,-1)[0])\n",
    "        losses_epoch_train.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    for x_example, y_example in test_loader:\n",
    "        y_pred, (h, c) = model(x_example, (h, c))\n",
    "    \n",
    "        loss = criterion(y_pred, y_example.long().reshape(1,-1)[0])\n",
    "        losses_epoch_test.append(loss.item())\n",
    "        \n",
    "        y_pred_test.extend(y_pred)\n",
    "        y_true_test.extend(list(y_example))\n",
    "    \n",
    "    train_losses.append(np.mean(losses_epoch_train))\n",
    "    test_losses.append(np.mean(losses_epoch_test))\n",
    "    y_pred_test = list(map(lambda x: torch.argmax(x).item(), y_pred_test))\n",
    "    y_true_test = list(map(lambda x: x.item(), torch.stack(y_true_test).reshape(-1,1)))\n",
    "    if i%5==0:\n",
    "        print(f'epoch {i}: train_loss = {np.mean(losses_epoch_train):.8f}, test_loss = {np.mean(losses_epoch_test):.8f}') \n",
    "        print('Точность на тестовой выборке:', accuracy_score(y_true_test, y_pred_test))\n",
    "        sample_lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gru():\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        inp = [2]\n",
    "        h = torch.zeros((1, 100))\n",
    "\n",
    "        output_name = ''\n",
    "\n",
    "        for i in range(vocab.max_seq_len):\n",
    "            output, h = model(inp, h)\n",
    "            idx = output.argmax().item()\n",
    "            if idx==3:\n",
    "                print(output_name)\n",
    "                return\n",
    "            output_name += vocab.idx_to_token[idx]\n",
    "            inp = [idx]\n",
    "        print(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss = 1.41278142, test_loss = 1.09595696\n",
      "Точность на тестовой выборке: 0.6749559082892416\n",
      "лина\n",
      "epoch 5: train_loss = 0.86101546, test_loss = 0.91060228\n",
      "Точность на тестовой выборке: 0.7109347442680776\n",
      "маня\n",
      "epoch 10: train_loss = 0.77384902, test_loss = 0.85285127\n",
      "Точность на тестовой выборке: 0.7268077601410935\n",
      "вилина\n",
      "epoch 15: train_loss = 0.73051078, test_loss = 0.83930373\n",
      "Точность на тестовой выборке: 0.7358024691358025\n",
      "маня\n",
      "epoch 20: train_loss = 0.69961925, test_loss = 0.82560103\n",
      "Точность на тестовой выборке: 0.7430335097001763\n",
      "марий\n",
      "epoch 25: train_loss = 0.68088812, test_loss = 0.81763531\n",
      "Точность на тестовой выборке: 0.7398589065255732\n",
      "милий\n",
      "epoch 30: train_loss = 0.66656392, test_loss = 0.80998477\n",
      "Точность на тестовой выборке: 0.7455026455026456\n",
      "мильяна\n",
      "epoch 35: train_loss = 0.65647179, test_loss = 0.80486397\n",
      "Точность на тестовой выборке: 0.7513227513227513\n",
      "васильян\n",
      "epoch 40: train_loss = 0.64841724, test_loss = 0.80283762\n",
      "Точность на тестовой выборке: 0.7419753086419754\n",
      "марьян\n",
      "epoch 45: train_loss = 0.63530817, test_loss = 0.82564824\n",
      "Точность на тестовой выборке: 0.7458553791887125\n",
      "антоника\n",
      "epoch 50: train_loss = 0.63498288, test_loss = 0.82455896\n",
      "Точность на тестовой выборке: 0.7470899470899471\n",
      "вастасий\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.rnn = nn.GRU(input_size = input_size, hidden_size=hidden_size, batch_first=True)\n",
    "    self.fc1 = nn.Linear(hidden_size, 100)\n",
    "    self.fc2 = nn.Linear(100, output_size)\n",
    "    self.emb = nn.Embedding(vocab.vocab_len, input_size)\n",
    "    \n",
    "  def forward(self, X, h):\n",
    "    X = self.emb(torch.LongTensor(X))\n",
    "    X, h = self.rnn(X, h)\n",
    "    h = h.detach()\n",
    "    if len(X.shape)==3:\n",
    "        X = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "    X = nn.ReLU()(self.fc1(F.dropout(X, 0.3)))\n",
    "    y_pred = self.fc2(F.dropout(X, 0.3))\n",
    "    return y_pred, h\n",
    "\n",
    "h = torch.zeros((1, 14, 100), requires_grad=True)\n",
    "model = Model(200, 100, vocab.vocab_len)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i in range(51):\n",
    "    model.train()\n",
    "    losses_epoch_train = []\n",
    "    losses_epoch_test = []\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "    for x_example, y_example in train_loader:\n",
    "        y_pred, h = model(x_example, h)\n",
    "    \n",
    "        loss = criterion(y_pred, y_example.long().reshape(1,-1)[0])\n",
    "        losses_epoch_train.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    for x_example, y_example in test_loader:\n",
    "        y_pred, h = model(x_example, h)\n",
    "    \n",
    "        loss = criterion(y_pred, y_example.long().reshape(1,-1)[0])\n",
    "        losses_epoch_test.append(loss.item())\n",
    "        \n",
    "        y_pred_test.extend(y_pred)\n",
    "        y_true_test.extend(list(y_example))\n",
    "    \n",
    "    train_losses.append(np.mean(losses_epoch_train))\n",
    "    test_losses.append(np.mean(losses_epoch_test))\n",
    "    y_pred_test = list(map(lambda x: torch.argmax(x).item(), y_pred_test))\n",
    "    y_true_test = list(map(lambda x: x.item(), torch.stack(y_true_test).reshape(-1,1)))\n",
    "    if i%5==0:\n",
    "        print(f'epoch {i}: train_loss = {np.mean(losses_epoch_train):.8f}, test_loss = {np.mean(losses_epoch_test):.8f}') \n",
    "        print('Точность на тестовой выборке:', accuracy_score(y_true_test, y_pred_test))\n",
    "        sample_gru()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJf5iaA2fOTM"
   },
   "source": [
    "## 2. Генерирование текста при помощи RNN\n",
    "\n",
    "2.1 Скачайте из интернета какое-нибудь художественное произведение\n",
    "  * Выбирайте достаточно крупное произведение, чтобы модель лучше обучалась;\n",
    "\n",
    "2.2 На основе выбранного произведения создайте датасет. \n",
    "\n",
    "Отличия от задачи 1:\n",
    "  * Токены <SOS>, `<EOS>` и `<UNK>` можно не добавлять;\n",
    "  * При создании датасета текст необходимо предварительно разбить на части. Выберите желаемую длину последовательности `seq_len` и разбейте текст на построки длины `seq_len` (можно без перекрытия, можно с небольшим перекрытием).\n",
    "\n",
    "2.3 Создайте и обучите модель для генерации текста\n",
    "  * Задача ставится точно так же как в 1.2;\n",
    "  * При необходимости можете применить:\n",
    "    * двухуровневые рекуррентные слои (`num_layers`=2)\n",
    "    * [обрезку градиентов](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "\n",
    "2.4 Напишите функцию, которая генерирует фрагмент текста при помощи обученной модели\n",
    "  * Процесс генерации начинается с небольшого фрагмента текста `prime`, выбранного вами (1-2 слова) \n",
    "  * Сначала вы пропускаете через модель токены из `prime` и генерируете на их основе скрытое состояние рекуррентного слоя `h_t`;\n",
    "  * После этого вы генерируете строку нужной длины аналогично 1.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER I\n",
      "In my younger and more vulnerable years my father gave me some advice that I’ve been turning over in my mind ever since.“Whenever you feel like criticizing any one,” he told me, “just remember that all the people in this world haven’t had the advantages that you’ve had.”He didn’t say any more, but we’ve always been unusually communicative in a reserved way, and I understood that he meant a great deal more than that. In consequence, I’m inclined to reserve all judgments, a habit that ha\n"
     ]
    }
   ],
   "source": [
    "f = open('The-Great-Gatsby.txt', 'r', encoding = 'utf8')\n",
    "data = f.read()\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "  def __init__(self, data):\n",
    "    self.idx_to_token = dict()\n",
    "    self.token_to_idx = dict()\n",
    "    self.token_to_idx['<PAD>'] = 0\n",
    "    self.idx_to_token[0] = '<PAD>'\n",
    "    self.token_to_idx['<SOS>'] = 1\n",
    "    self.idx_to_token[1] = '<SOS>'\n",
    "    self.max_seq_len = 0\n",
    "    k = 2\n",
    "    for s in data.lower():\n",
    "        if s not in self.token_to_idx:\n",
    "            self.token_to_idx[s] = k\n",
    "            self.idx_to_token[k] = s\n",
    "            k+=1\n",
    "    self.max_seq_len = 140\n",
    "    self.vocab_len = len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "  def __init__(self, vocab):\n",
    "    self.vocab = vocab\n",
    "\n",
    "  def vectorize(self, surname):\n",
    "    s_rep = []\n",
    "    i = 0\n",
    "    text = data.lower()\n",
    "    while i < len(text)-vocab.max_seq_len:\n",
    "        rep = text[i:i+vocab.max_seq_len-2]\n",
    "        rep = rep[:rep.rfind(' ')+1]\n",
    "        rep_new = []\n",
    "        for r in rep:\n",
    "            rep_new.append(vocab.token_to_idx[r])\n",
    "        a = [vocab.token_to_idx['<SOS>']]+rep_new+[0]*(vocab.max_seq_len-len(rep_new)-2)\n",
    "        s_rep.append(torch.tensor(a))\n",
    "        i+=len(rep)\n",
    "    self.X_rep = torch.stack(s_rep)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X_rep)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample = self.X_rep[idx], torch.tensor(list(self.X_rep[idx][1:])+[0])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(vocab)\n",
    "dataset.vectorize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 10, 12,  9, 13, 14,  9, 14,\n",
       "         15, 16, 12, 17,  7,  8,  9,  4, 12, 18,  9, 13, 15,  8,  7,  9, 19, 16,\n",
       "         20, 12,  7,  8,  4, 21, 20,  7,  9, 14,  7,  4,  8, 22,  9, 13, 14,  9,\n",
       "         23,  4,  6,  3,  7,  8,  9, 17,  4, 19,  7,  9, 13,  7,  9, 22, 15, 13,\n",
       "          7,  9,  4, 18, 19, 10,  2,  7,  9,  6,  3,  4,  6,  9, 10, 24, 19,  7,\n",
       "          9, 21,  7,  7, 12,  9,  6, 16,  8, 12, 10, 12, 17,  9, 15, 19,  7,  8,\n",
       "          9, 10, 12,  9, 13, 14,  9, 13, 10, 12, 18,  9,  7, 19,  7,  8,  9,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 10, 12,  9, 13, 14,  9, 14, 15,\n",
       "         16, 12, 17,  7,  8,  9,  4, 12, 18,  9, 13, 15,  8,  7,  9, 19, 16, 20,\n",
       "         12,  7,  8,  4, 21, 20,  7,  9, 14,  7,  4,  8, 22,  9, 13, 14,  9, 23,\n",
       "          4,  6,  3,  7,  8,  9, 17,  4, 19,  7,  9, 13,  7,  9, 22, 15, 13,  7,\n",
       "          9,  4, 18, 19, 10,  2,  7,  9,  6,  3,  4,  6,  9, 10, 24, 19,  7,  9,\n",
       "         21,  7,  7, 12,  9,  6, 16,  8, 12, 10, 12, 17,  9, 15, 19,  7,  8,  9,\n",
       "         10, 12,  9, 13, 14,  9, 13, 10, 12, 18,  9,  7, 19,  7,  8,  9,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = next(iter(dataset))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_train, batch_size = 4, shuffle = True)\n",
    "test_loader = DataLoader(data_test, batch_size = 4, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size = 20, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rnn(model):\n",
    "    with torch.no_grad():  \n",
    "        inp = [10, 9, 20, 15, 19, 7]\n",
    "        hidden = torch.zeros((2, 400))\n",
    "\n",
    "        output_name = 'i love'\n",
    "\n",
    "        for i in range(vocab.max_seq_len):\n",
    "            output, hidden = model(inp, hidden)\n",
    "            idx = output[-1].argmax().item()\n",
    "            if idx == 0:\n",
    "                break\n",
    "            output_name += vocab.idx_to_token[idx]\n",
    "            inp.append(idx)\n",
    "        print(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss = 2.31655732\n",
      "Точность: 0.34703420936720014\n",
      "i lover and the was and the was and the was and the was and the was and the was and the was and the was and the was and the was and the was and th\n",
      "epoch 3: train_loss = 1.64835377\n",
      "Точность: 0.5038503890764939\n",
      "i love to the said to the said to the said to the should and the was a little and the said to the could and the said to he was a little and the st\n",
      "epoch 6: train_loss = 1.48828254\n",
      "Точность: 0.5483849654969901\n",
      "i loved the shook and the started to the started to the sunder the stand in the starting the porth and the starting the should have the supper of \n",
      "epoch 9: train_loss = 1.40411224\n",
      "Точность: 0.5717736015269417\n",
      "i loved him and the car that i was a startled to a little before the corner of the corner of the surprised the counted to the stranged and the cou\n",
      "epoch 12: train_loss = 1.34362097\n",
      "Точность: 0.5877294083100866\n",
      "i love the stations of the world \n",
      "epoch 15: train_loss = 1.30066814\n",
      "Точность: 0.5985905153428278\n",
      "i love in the sunder of the station of the sound of the station of the stared to the side of the stared at the start of the started to the door of\n",
      "epoch 18: train_loss = 1.26402364\n",
      "Точность: 0.6089891352224343\n",
      "i love the station and then the silver and the street of the station of the steps and the steps and the steps and the station of the stairs. “i do\n",
      "epoch 21: train_loss = 1.23437154\n",
      "Точность: 0.6173726325062399\n",
      "i love to the door. then the morning at the sound. the door and then in the sound. the man in a moment the man was a back to the concealed by the \n",
      "epoch 24: train_loss = 1.20625244\n",
      "Точность: 0.6245778887094406\n",
      "i love to the world.”“i don’t you about him to gatsby’s house and the sunlight of the sound. the dark around an hour. i was a straw that he was a \n",
      "epoch 27: train_loss = 1.18228276\n",
      "Точность: 0.62992585523418\n",
      "i love to see you another went to go to tow and he was a moment of course i was continued out of the summer was standing on the side of the second\n",
      "epoch 30: train_loss = 1.15843094\n",
      "Точность: 0.63731463808545\n",
      "i love the sharp of the darkness. i was so long again, and then there were stared at the sound of the shadow and then the short before he took her\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.rnn = nn.RNN(input_size = input_size, hidden_size=hidden_size, batch_first=True, num_layers=2, dropout = 0.3)\n",
    "#     self.fc1 = nn.Linear(hidden_size, 100)\n",
    "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    self.emb = nn.Embedding(vocab.vocab_len, input_size)\n",
    "    \n",
    "  def forward(self, X, h):\n",
    "    X = self.emb(torch.LongTensor(X))\n",
    "    X, h = self.rnn(X, h)\n",
    "    h = h.detach()\n",
    "    if len(X.shape)==3:\n",
    "        X = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "#     X = nn.Tanh()(self.fc1(F.dropout(X, 0.2)))\n",
    "    y_pred = self.fc2(X)\n",
    "    return y_pred, h\n",
    "\n",
    "h = torch.zeros((2, 20, 400), requires_grad=True)\n",
    "model = Model(100, 400, vocab.vocab_len)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i in range(31):\n",
    "    model.train()\n",
    "    losses_epoch_train = []\n",
    "    losses_epoch_test = []\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "    for x_example, y_example in data_loader:\n",
    "        y_pred, h = model(x_example, h)\n",
    "    \n",
    "        loss = criterion(y_pred, y_example.long().reshape(1,-1)[0])\n",
    "        losses_epoch_train.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred_test.extend(y_pred)\n",
    "        y_true_test.extend(list(y_example))\n",
    "        \n",
    "    \n",
    "    train_losses.append(np.mean(losses_epoch_train))\n",
    "    y_pred_test = list(map(lambda x: torch.argmax(x).item(), y_pred_test))\n",
    "    y_true_test = list(map(lambda x: x.item(), torch.stack(y_true_test).reshape(-1,1)))\n",
    "    if i%3==0:\n",
    "        print(f'epoch {i}: train_loss = {np.mean(losses_epoch_train):.8f}') \n",
    "        print('Точность:', accuracy_score(y_true_test, y_pred_test))\n",
    "        sample_rnn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love the short blowned to the sound of the shore and then the sharp of the stars. i was some one hars and the time to the sour of the shadow of \n",
      "i love there was the should got to see her head and then the short of the summer and the should over the course of the sound of the shadow of the \n",
      "i love the short of the stranger of the sharp of the sea-change in the sun and to see him so i didn’t know what you want to get up to the sunlight\n",
      "i love to the sharp of the sound of the sidew and the country of the sound of the strench of the shadow and then there was a green light the sound\n",
      "i love the stared to him that the count of the side, and then to the door and started to see the sound of the sun of the country of the convertain\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    sample_rnn(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "я люблю короткий дуновение под шум берега, а затем резкий блеск звезд. я был кем-то из харса, и время для кислой тени \n",
    "\n",
    "мне нравится, что был должен увидеть ее голову, а затем короткий конец лета, и они должны были в течение звука тени \n",
    "\n",
    "я люблю короткометражку \"незнакомец на берегу моря\" - измениться на солнце и увидеть его так, что я не знал, чего ты хочешь, чтобы встать навстречу солнечному свету.\n",
    "\n",
    "я люблю резкий звук стороны и страны, звук силы тени, а потом появился зеленый свет, звук\n",
    "\n",
    "я люблю смотреть на него, что граф сбоку, а потом на дверь и начал видеть звук солнца страны обращенных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_lstm(model):\n",
    "    with torch.no_grad():  \n",
    "        inp = [10, 9, 20, 15, 19, 7]\n",
    "        h = torch.zeros((2, 300))\n",
    "        c = torch.zeros((2, 300))\n",
    "\n",
    "        output_name = 'i love'\n",
    "\n",
    "        for i in range(vocab.max_seq_len):\n",
    "            prev_state = (h,c)\n",
    "            output, (h, c) = model(inp, prev_state)\n",
    "            idx = output[-1].argmax().item()\n",
    "            if idx == 0:\n",
    "                break\n",
    "            output_name += vocab.idx_to_token[idx]\n",
    "            inp.append(idx)\n",
    "        print(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss = 2.74131282\n",
      "Точность на тестовой выборке: 0.2534906768462781\n",
      "i love the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "epoch 3: train_loss = 1.80450139\n",
      "Точность на тестовой выборке: 0.4642123036264866\n",
      "i love the she was and the said the said the somether and the said the said the said the she said the started the said the said and the said the s\n",
      "epoch 6: train_loss = 1.58672331\n",
      "Точность на тестовой выборке: 0.5207862281603289\n",
      "i love the short of the shoulder to the supprised the start the start of the shoulder and she was a little starting the started the staring the st\n",
      "epoch 9: train_loss = 1.46710845\n",
      "Точность на тестовой выборке: 0.5552672147995888\n",
      "i loved the cold see of the shoulder and the back and she was a something to the counter of the course of the short of the sound of the counter an\n",
      "epoch 12: train_loss = 1.38361091\n",
      "Точность на тестовой выборке: 0.5779731316987227\n",
      "i loved to me and the sunger and some of the started to the sun of the started and the shot of thing to the startless and started and some of the \n",
      "epoch 15: train_loss = 1.31621183\n",
      "Точность на тестовой выборке: 0.5958009102921744\n",
      "i love it was a stranger and the stood and the stopped and the starder of the started to the door and the shook and the started to the standing an\n",
      "epoch 18: train_loss = 1.26177713\n",
      "Точность на тестовой выборке: 0.6102334458963441\n",
      "i loved you in the motor and the poor of the man in the door.”“i’m going to the sound.”“you went to the sunting is the man was a station of the do\n",
      "epoch 21: train_loss = 1.21503382\n",
      "Точность на тестовой выборке: 0.6233078843048011\n",
      "i loved you and i was a sound. i was a strange of the states of the sight of the stand of the sun of the sungestion of the sun in a staring and li\n",
      "epoch 24: train_loss = 1.17124944\n",
      "Точность на тестовой выборке: 0.6346828659521363\n",
      "i loved you a little butler to see you are the montenegron and the confusion of a moment the step and conversation he was a small past and the con\n",
      "epoch 27: train_loss = 1.12923612\n",
      "Точность на тестовой выборке: 0.6466634855381002\n",
      "i loves on the door.”“we’re going to some of the darkness of the movements of the \n",
      "epoch 30: train_loss = 1.09144427\n",
      "Точность на тестовой выборке: 0.6576347085596829\n",
      "i loved you and there was a sunge of the man who had been arrived and the telemone of the steps and stood on the steps and the starting and then t\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.rnn = nn.LSTM(input_size = input_size, hidden_size=hidden_size, batch_first=True, num_layers=2, dropout = 0.2)\n",
    "#     self.fc1 = nn.Linear(hidden_size, 100)\n",
    "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    self.emb = nn.Embedding(vocab.vocab_len, input_size)\n",
    "    \n",
    "  def forward(self, X, prev_state):\n",
    "    X = self.emb(torch.LongTensor(X))\n",
    "    X, (h, c) = self.rnn(X, prev_state)\n",
    "    h = h.detach()\n",
    "    c = c.detach()\n",
    "    if len(X.shape)==3:\n",
    "        X = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "#     X = nn.Tanh()(self.fc1(F.dropout(X, 0.2)))\n",
    "    y_pred = self.fc2(X)\n",
    "    return y_pred, (h, c)\n",
    "\n",
    "h = torch.zeros((2, 20, 300), requires_grad=True)\n",
    "c = torch.zeros((2, 20, 300), requires_grad=True)\n",
    "model = Model(100, 300, vocab.vocab_len)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i in range(31):\n",
    "    model.train()\n",
    "    losses_epoch_train = []\n",
    "    losses_epoch_test = []\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "    for x_example, y_example in data_loader:\n",
    "        y_pred, (h, c) = model(x_example, (h, c))\n",
    "    \n",
    "        loss = criterion(y_pred, y_example.long().reshape(1,-1)[0])\n",
    "        losses_epoch_train.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred_test.extend(y_pred)\n",
    "        y_true_test.extend(list(y_example))\n",
    "                \n",
    "        \n",
    "    \n",
    "    train_losses.append(np.mean(losses_epoch_train))\n",
    "    y_pred_test = list(map(lambda x: torch.argmax(x).item(), y_pred_test))\n",
    "    y_true_test = list(map(lambda x: x.item(), torch.stack(y_true_test).reshape(-1,1)))\n",
    "    if i%3==0:\n",
    "        print(f'epoch {i}: train_loss = {np.mean(losses_epoch_train):.8f}') \n",
    "        print('Точность на тестовой выборке:', accuracy_score(y_true_test, y_pred_test))\n",
    "        sample_lstm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i loved him in the secret man was any other times and the sun of the steps and started to the station that he had been a sunges of the stairs and \n",
      "i loved you about the parties and the sun of the man \n",
      "i loved him.”“i don’t know what i was all this in the sunlight that they’re all the matter of a moment the past.”“i think he was a great figures o\n",
      "i loved her in the summer.”“i think he was a great road concertative neightened to \n",
      "i loved herself to get the station of the man who had been there with a small \n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    sample_lstm(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "я любила его в \"тайном человеке\", \"в любое другое время\" и \"солнце ступеней\", и отправилась на станцию, где он был \"солнцем ступеней\", и \n",
    "\n",
    "я любил тебя за вечеринки и солнце этого человека \n",
    "\n",
    "я любила его”. “я не знаю, кем я была все это время при солнечном свете, что все это дело мгновения прошлого”. “я думаю, что он был великим деятелем о\n",
    "\n",
    "я любил ее летом”. “Я думаю, что он был отличным дорожным концертником, приближенным к \n",
    "\n",
    "я любила саму себя, чтобы получить место человека, который был там с маленьким"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'Model2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gru(model):\n",
    "    with torch.no_grad():  \n",
    "        inp = [10, 9, 20, 15, 19, 7]\n",
    "        hidden = torch.zeros((2, 300))\n",
    "\n",
    "        output_name = 'i love'\n",
    "\n",
    "        for i in range(vocab.max_seq_len):\n",
    "            output, hidden = model(inp, hidden)\n",
    "            idx = output[-1].argmax().item()\n",
    "            if idx == 0:\n",
    "                break\n",
    "            output_name += vocab.idx_to_token[idx]\n",
    "            inp.append(idx)\n",
    "        print(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss = 2.47988492\n",
      "Точность: 0.3077998825429452\n",
      "i love the the was the the was the was the to to the the was the was the the the the the was and the the was the the the was and the the was to th\n",
      "epoch 3: train_loss = 1.59948379\n",
      "Точность: 0.5147261782410806\n",
      "i loved and the contiled and the start of the was a minute and the really and the conternoon of the starded at the started the said the started th\n",
      "epoch 6: train_loss = 1.42178104\n",
      "Точность: 0.5635002202319777\n",
      "i love and started to the started to see the started and saw the stand of the started to the started at the started at the started and the started\n",
      "epoch 9: train_loss = 1.32629948\n",
      "Точность: 0.5894398766700925\n",
      "i loved him to the train the strange the strange the startled to the train to the train the strange of the stretting the sunderstand i don’t know \n",
      "epoch 12: train_loss = 1.25755998\n",
      "Точность: 0.6090258405520481\n",
      "i loved him in the sunformality of the single broke of the sunder and the concertion that he was a sort of the strange and the dark and the should\n",
      "epoch 15: train_loss = 1.20743818\n",
      "Точность: 0.6217809425928645\n",
      "i loved him for a moment i was so interested to the station of the side of the stand. the bay was side of the steps and sat down and the stars in \n",
      "epoch 18: train_loss = 1.16171314\n",
      "Точность: 0.6344663045074145\n",
      "i love to his house,” said tom buchanan and the couch and continued tom buchanan and the controlled at the strange with a convinced to the \n",
      "epoch 21: train_loss = 1.12224651\n",
      "Точность: 0.6456100425781823\n",
      "i love the stars. the only complete the strange of the \n",
      "epoch 24: train_loss = 1.08749927\n",
      "Точность: 0.6539935398619879\n",
      "i loved him and \n",
      "epoch 27: train_loss = 1.05276501\n",
      "Точность: 0.6639516957862281\n",
      "i loved her \n",
      "epoch 30: train_loss = 1.02270443\n",
      "Точность: 0.6731610629863456\n",
      "i love in the \n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.rnn = nn.GRU(input_size = input_size, hidden_size=hidden_size, batch_first=True, num_layers=2, dropout = 0.3)\n",
    "#     self.fc1 = nn.Linear(hidden_size, 100)\n",
    "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    self.emb = nn.Embedding(vocab.vocab_len, input_size)\n",
    "    \n",
    "  def forward(self, X, h):\n",
    "    X = self.emb(torch.LongTensor(X))\n",
    "    X, h = self.rnn(X, h)\n",
    "    h = h.detach()\n",
    "    if len(X.shape)==3:\n",
    "        X = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "#     X = nn.Tanh()(self.fc1(F.dropout(X, 0.2)))\n",
    "    y_pred = self.fc2(X)\n",
    "    return y_pred, h\n",
    "\n",
    "h = torch.zeros((2, 20, 300), requires_grad=True)\n",
    "model = Model(100, 300, vocab.vocab_len)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i in range(31):\n",
    "    model.train()\n",
    "    losses_epoch_train = []\n",
    "    losses_epoch_test = []\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "    for x_example, y_example in data_loader:\n",
    "        y_pred, h = model(x_example, h)\n",
    "    \n",
    "        loss = criterion(y_pred, y_example.long().reshape(1,-1)[0])\n",
    "        losses_epoch_train.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred_test.extend(y_pred)\n",
    "        y_true_test.extend(list(y_example))\n",
    "        \n",
    "    \n",
    "    train_losses.append(np.mean(losses_epoch_train))\n",
    "    y_pred_test = list(map(lambda x: torch.argmax(x).item(), y_pred_test))\n",
    "    y_true_test = list(map(lambda x: x.item(), torch.stack(y_true_test).reshape(-1,1)))\n",
    "    if i%3==0:\n",
    "        print(f'epoch {i}: train_loss = {np.mean(losses_epoch_train):.8f}') \n",
    "        print('Точность:', accuracy_score(y_true_test, y_pred_test))\n",
    "        sample_gru(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love that he was \n",
      "i love the statement \n",
      "i love a chair \n",
      "i loved him that \n",
      "i love that the \n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    sample_gru(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'Model3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_lstm(model):\n",
    "    with torch.no_grad():  \n",
    "        inp = [10, 9, 20, 15, 19, 7]\n",
    "        h = torch.zeros((2, 300))\n",
    "        c = torch.zeros((2, 300))\n",
    "\n",
    "        output_name = 'i love'\n",
    "\n",
    "        for i in range(vocab.max_seq_len):\n",
    "            prev_state = (h,c)\n",
    "            output, (h, c) = model(inp, prev_state)\n",
    "            idx = output[-1].argmax().item()\n",
    "            if idx == 0:\n",
    "                break\n",
    "            output_name += vocab.idx_to_token[idx]\n",
    "            inp.append(idx)\n",
    "        print(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss = 2.60666822\n",
      "Точность на тестовой выборке: 0.28350095433857\n",
      "i love the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "epoch 3: train_loss = 1.72257159\n",
      "Точность на тестовой выборке: 0.48535824401703126\n",
      "i loved the conter a monted the cars of the come and the compless of the conting the said and the said the said the came a moment of the said the \n",
      "epoch 6: train_loss = 1.53530796\n",
      "Точность на тестовой выборке: 0.5348627220672442\n",
      "i love the compleated the come of the started the conterney of the complete and the contering to the could start of the montered the complest of t\n",
      "epoch 9: train_loss = 1.42475697\n",
      "Точность на тестовой выборке: 0.5646050506533549\n",
      "i loved the start of the dream of a moment of the strange of the stared the sunders of the started the strange of the startled the contruct of the\n",
      "epoch 12: train_loss = 1.34903662\n",
      "Точность на тестовой выборке: 0.58485171046836\n",
      "i loved a moment of the stared to see the sun his hand of the starting to the porch of the start of the rest of the stars of the started to the st\n",
      "epoch 15: train_loss = 1.29012264\n",
      "Точность на тестовой выборке: 0.6014792247834385\n",
      "i loved him and the stars of the door. the bare of the stars and the stars and the stranger of the stars of the state of the stars of the sun of t\n",
      "epoch 18: train_loss = 1.24035478\n",
      "Точность на тестовой выборке: 0.6152217001908677\n",
      "i loved a moment the stars and the side of the side of the station that the silver stopped and the corner and the station that the starting a litt\n",
      "epoch 21: train_loss = 1.19651879\n",
      "Точность на тестовой выборке: 0.626225958009103\n",
      "i loved him to the strained at the strange at the station of the room was a strangest of the strangest of the station that he was a strange that h\n",
      "epoch 24: train_loss = 1.15493729\n",
      "Точность на тестовой выборке: 0.638360739979445\n",
      "i loved him and the street of the single considered of the sound of a strained at the room.“i’m going to town the pool.”“i’ve got to go to mean i \n",
      "epoch 27: train_loss = 1.11681500\n",
      "Точность на тестовой выборке: 0.6496476288357069\n",
      "i loved him for a moment the station of the station of the shadows of the sound of the station of the sunlight of the sideways and the steps and s\n",
      "epoch 30: train_loss = 1.07982422\n",
      "Точность на тестовой выборке: 0.6592387314638085\n",
      "i loved him and said to see my head and the \n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.rnn = nn.LSTM(input_size = input_size, hidden_size=hidden_size, batch_first=True, num_layers=2, dropout = 0.2)\n",
    "#     self.fc1 = nn.Linear(hidden_size, 100)\n",
    "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    self.emb = nn.Embedding(vocab.vocab_len, input_size)\n",
    "    \n",
    "  def forward(self, X, prev_state):\n",
    "    X = self.emb(torch.LongTensor(X))\n",
    "    X, (h, c) = self.rnn(X, prev_state)\n",
    "    h = h.detach()\n",
    "    c = c.detach()\n",
    "    if len(X.shape)==3:\n",
    "        X = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "#     X = nn.Tanh()(self.fc1(F.dropout(X, 0.2)))\n",
    "    y_pred = self.fc2(X)\n",
    "    return y_pred, (h, c)\n",
    "\n",
    "h = torch.zeros((2, 20, 300), requires_grad=True)\n",
    "c = torch.zeros((2, 20, 300), requires_grad=True)\n",
    "model = Model(250, 300, vocab.vocab_len)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i in range(31):\n",
    "    model.train()\n",
    "    losses_epoch_train = []\n",
    "    losses_epoch_test = []\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "    for x_example, y_example in data_loader:\n",
    "        y_pred, (h, c) = model(x_example, (h, c))\n",
    "    \n",
    "        loss = criterion(y_pred, y_example.long().reshape(1,-1)[0])\n",
    "        losses_epoch_train.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred_test.extend(y_pred)\n",
    "        y_true_test.extend(list(y_example))\n",
    "                \n",
    "        \n",
    "    \n",
    "    train_losses.append(np.mean(losses_epoch_train))\n",
    "    y_pred_test = list(map(lambda x: torch.argmax(x).item(), y_pred_test))\n",
    "    y_true_test = list(map(lambda x: x.item(), torch.stack(y_true_test).reshape(-1,1)))\n",
    "    if i%3==0:\n",
    "        print(f'epoch {i}: train_loss = {np.mean(losses_epoch_train):.8f}') \n",
    "        print('Точность на тестовой выборке:', accuracy_score(y_true_test, y_pred_test))\n",
    "        sample_lstm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i loved him to the cournerauld realized that he was an expression that he was a great distance that he was a great floor of the country of the con\n",
      "i loved him and the coupe reached the policeman book and the contracted that the sun staring the star. at his head and then arm only the cold back\n",
      "i loved him and the control of the coupe and stared at the stars. the trains with the sunlight and the coupe and the contracted the controlled out\n",
      "i loved her because i was something to the world and the \n",
      "i loved him and he was a great suppressions of the warm of the station of the sun of the continually and the couch and say to the \n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    sample_lstm(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "я любил его до такой степени, что Курнеро понял, что он был выражением, что он был на большом расстоянии, что он был великим полом страны кон\n",
    "\n",
    "я любила его, и пара дошла до книги полицейского и до того, что солнце смотрит на звезду. на его голову, а затем на руку, только холодная спина\n",
    "\n",
    "я любила его, любила управлять купе и смотрела на звезды. поезда с солнечным светом, купе, контрактом, контролируемым выходом\n",
    "\n",
    "я любил ее, потому что я был чем-то для мира и\n",
    "\n",
    "я любила его, и он был великим подавителем тепла станции, солнца, кровати и, скажем,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss = 1.06851125\n",
      "Точность на тестовой выборке: 0.6627954779033916\n",
      "i loved him in the steps and supper of the sound of the \n",
      "epoch 3: train_loss = 1.03295470\n",
      "Точность на тестовой выборке: 0.6738694758478931\n",
      "i loved the side of the world was a strange to the sound of a sort of this possibility of a short of the \n",
      "epoch 6: train_loss = 0.99957224\n",
      "Точность на тестовой выборке: 0.6833284392893848\n",
      "i loved his hand and the butler way of the \n",
      "epoch 9: train_loss = 0.96836158\n",
      "Точность на тестовой выборке: 0.6932645720158567\n",
      "i loved in a contralto invite a strain of his position of his hands so little ground. i was a straining into the sunlight and started to be a chai\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model.train()\n",
    "    losses_epoch_train = []\n",
    "    losses_epoch_test = []\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "    for x_example, y_example in data_loader:\n",
    "        y_pred, (h, c) = model(x_example, (h, c))\n",
    "    \n",
    "        loss = criterion(y_pred, y_example.long().reshape(1,-1)[0])\n",
    "        losses_epoch_train.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred_test.extend(y_pred)\n",
    "        y_true_test.extend(list(y_example))\n",
    "                \n",
    "        \n",
    "    \n",
    "    train_losses.append(np.mean(losses_epoch_train))\n",
    "    y_pred_test = list(map(lambda x: torch.argmax(x).item(), y_pred_test))\n",
    "    y_true_test = list(map(lambda x: x.item(), torch.stack(y_true_test).reshape(-1,1)))\n",
    "    if i%3==0:\n",
    "        print(f'epoch {i}: train_loss = {np.mean(losses_epoch_train):.8f}') \n",
    "        print('Точность на тестовой выборке:', accuracy_score(y_true_test, y_pred_test))\n",
    "        sample_lstm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i loved into the garage with a strain to the door of the \n",
      "i loved him and then at the car.“i love here, old sport,” said tom and the chair. the only hands and the strange to the sundan cold back and the c\n",
      "i loved in the door. she had dressed at the straining into the sidewalk and the cars there were all the time in a strangest of the correction of h\n",
      "i loved in a sound. then i had not another warm stranger and daisy was startlingly aware of the counter of a solitary leaves in a while in the str\n",
      "i loved into the door. and the reason in the room where the sun stars. at the garage with a sort of things and the lady \n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    sample_lstm(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "я вошел в гараж с усилием, направившись к двери\n",
    "\n",
    "я полюбил его, а потом в машине. “Мне нравится здесь, старина”, - сказал Том и кресло. единственные руки и странная для сундана холодная спина и с\n",
    "\n",
    "я любил в дверях. она оделась, вжимаясь в тротуар, и машины там все время были в самом странном из исправлений h\n",
    "\n",
    "я любил в звуке. тогда у меня не было другого теплого незнакомца, и Дейзи поразительно осознавала, что через некоторое время на улице появляется счетчик одиноких листьев\n",
    "\n",
    "мне нравилось входить в дверь. и причина в комнате, где солнце светит звездами. в гараже с какими-то вещами и дамой\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss = 0.95676865\n",
      "Точность на тестовой выборке: 0.6959844369402437\n",
      "i loved him that he was a stranger and the \n",
      "epoch 3: train_loss = 0.92470929\n",
      "Точность на тестовой выборке: 0.7048854793716047\n",
      "i loved him out over the sunlight and started out on the stars. the word was the station in his hand was the strange to the sound of a man who had\n",
      "epoch 6: train_loss = 0.89554370\n",
      "Точность на тестовой выборке: 0.7137388048744677\n",
      "i loved him on the sound of a commotion and the strained street and to the \n",
      "epoch 9: train_loss = 0.86840252\n",
      "Точность на тестовой выборке: 0.7216231096755249\n",
      "i loved daisy and in a waved \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model.train()\n",
    "    losses_epoch_train = []\n",
    "    losses_epoch_test = []\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "    for x_example, y_example in data_loader:\n",
    "        y_pred, (h, c) = model(x_example, (h, c))\n",
    "    \n",
    "        loss = criterion(y_pred, y_example.long().reshape(1,-1)[0])\n",
    "        losses_epoch_train.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred_test.extend(y_pred)\n",
    "        y_true_test.extend(list(y_example))\n",
    "                \n",
    "        \n",
    "    \n",
    "    train_losses.append(np.mean(losses_epoch_train))\n",
    "    y_pred_test = list(map(lambda x: torch.argmax(x).item(), y_pred_test))\n",
    "    y_true_test = list(map(lambda x: x.item(), torch.stack(y_true_test).reshape(-1,1)))\n",
    "    if i%3==0:\n",
    "        print(f'epoch {i}: train_loss = {np.mean(losses_epoch_train):.8f}') \n",
    "        print('Точность на тестовой выборке:', accuracy_score(y_true_test, y_pred_test))\n",
    "        sample_lstm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i loved every must have been madiessill and she looked around at the sunlight the steps and started out and took the book, and had a strange hour \n",
      "i loved every must have been college.“i know in the man was to think.”“you’re a club to tell you and stopped on the poor.”“oh, i think of course i\n",
      "i loved me on the sunlight and supprosping on the stables.“i’m going to town, and she went to her hands to the past.”“hello!” i asked.“of course i\n",
      "i loved by a subden’s bridge,” he said. “it was a friend of me to myself.”“come on,” she said suddenly.“the promise to her about the other car.”“s\n",
      "i loved down the straightformororrow \n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    sample_lstm(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "мне нравилось все, что должно было быть Мэдисон, и она оглянулась на солнечный свет, на ступеньки, вышла, взяла книгу и провела странный час \n",
    "\n",
    "я любила все, что должно было быть в колледже.“я знаю, что в мужчине было думать”. “Ты дубина, чтобы сказать тебе и остановился на бедных”. “О, я думаю, конечно, я\n",
    "\n",
    "я любил себя на солнечном свете и прогуливался по конюшням. “Я еду в город, а она взяла себя в руки и отправилась в прошлое”. “Привет!” - спросил я.“конечно, я\n",
    "\n",
    "я любил у моста Субдена”, - сказал он. “это был мой друг для меня самой”. “Давай”, - внезапно сказала она.”обещание, данное ей насчет другой машины“.\"с\n",
    "\n",
    "я любил прямолинейную форму или печаль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss = 0.85924008\n",
      "Точность на тестовой выборке: 0.7252349141095287\n",
      "i loved the pool. once of the contrasting clerk of men shaderable stretched the policeman car was \n",
      "epoch 3: train_loss = 0.82954785\n",
      "Точность на тестовой выборке: 0.7332036411686977\n",
      "i loved echooping tragically at the same barbeas of the \n",
      "epoch 6: train_loss = 0.80747157\n",
      "Точность на тестовой выборке: 0.7398766700924975\n",
      "i loved easien, and then i was still asherined as the \n",
      "epoch 9: train_loss = 0.78405950\n",
      "Точность на тестовой выборке: 0.7474673322566436\n",
      "i loved eyes. i had enough to do it?” he remarked at the subject of the sunlight hard. it had occurred to me that he was a gold \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model.train()\n",
    "    losses_epoch_train = []\n",
    "    losses_epoch_test = []\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "    for x_example, y_example in data_loader:\n",
    "        y_pred, (h, c) = model(x_example, (h, c))\n",
    "    \n",
    "        loss = criterion(y_pred, y_example.long().reshape(1,-1)[0])\n",
    "        losses_epoch_train.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred_test.extend(y_pred)\n",
    "        y_true_test.extend(list(y_example))\n",
    "                \n",
    "        \n",
    "    \n",
    "    train_losses.append(np.mean(losses_epoch_train))\n",
    "    y_pred_test = list(map(lambda x: torch.argmax(x).item(), y_pred_test))\n",
    "    y_true_test = list(map(lambda x: x.item(), torch.stack(y_true_test).reshape(-1,1)))\n",
    "    if i%3==0:\n",
    "        print(f'epoch {i}: train_loss = {np.mean(losses_epoch_train):.8f}') \n",
    "        print('Точность на тестовой выборке:', accuracy_score(y_true_test, y_pred_test))\n",
    "        sample_lstm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i loved each other and said gatsby, with a sort of this gray, and started up to some wild bridy.“looks here, and i was a matter of a man i knew wh\n",
      "i loved eagerine at the continent, and i went over the \n",
      "i loved delicately to the station was gate.“i can’t say about it. it’s too long island,” he said with a polite toward the porch i had not been mor\n",
      "i loved every money. i had a small cheerful supercilious \n",
      "i loved eyes. she was sitting on the couch and the dead sight and \n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    sample_lstm(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "я любил друг друга и сказал “Гэтсби\", с чем-то вроде этого грея, и начал с какой-то дикой брайди.\"Смотрите сюда, и я был человеком, которого я знал, который\n",
    "\n",
    "мне очень понравился игерин на континенте, и я прошел через \n",
    "\n",
    "я любил деликатно, чтобы на станции были ворота.“я не могу сказать об этом. это слишком Лонг-айленд, - сказал он с вежливым видом в сторону крыльца, на котором я не был раньше.\n",
    "\n",
    "я любил все деньги. у меня был маленький веселый надменный \n",
    "\n",
    "я любила глаза. она сидела на диване, и мертвый вид, и"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOt/b54+xoKtnmvuSlliKDY",
   "collapsed_sections": [],
   "name": "blank__08_rnn_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
